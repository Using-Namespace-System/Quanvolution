{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 21:44:52.050462: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-11 21:44:52.050515: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-11 21:44:52.051601: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-11 21:44:52.057730: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-11 21:44:52.791335: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8a7fc43a9712319e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8a7fc43a9712319e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates import RandomLayers\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "'''\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "'''\n",
    "%tensorboard --logdir logs/scalars/\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir= \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_grads=True\n",
    "    )\n",
    "\n",
    "checkpoint_path = \"./training_2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "   checkpoint_path, verbose=1, save_weights_only=True,\n",
    "   # Save weights, every epoch.\n",
    "   save_freq='epoch')\n",
    "\n",
    "\n",
    "\n",
    "#tf.get_logger().setLevel('INFO')\n",
    "\n",
    "n_epochs = 30   # Number of optimization epochs\n",
    "n_layers = 1    # Number of random layers\n",
    "n_train = 200    # Size of the train dataset\n",
    "n_test = 120     # Size of the test dataset\n",
    "n_batches = 4     # Size of the batches\n",
    "\n",
    "np.random.seed(0)           # Seed for NumPy random number generator\n",
    "tf.random.set_seed(0)       # Seed for TensorFlow random number generator\n",
    "\n",
    "\n",
    "\n",
    "mnist_dataset = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist_dataset.load_data()\n",
    "\n",
    "\n",
    "\n",
    "# Reduce dataset size\n",
    "train_images = train_images[:n_train]\n",
    "train_labels = train_labels[:n_train]\n",
    "test_images = test_images[:n_test]\n",
    "test_labels = test_labels[:n_test]\n",
    "\n",
    "tf.config.get_visible_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize pixel values within 0 and 1\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255\n",
    "\n",
    "# Add extra dimension for convolution channels\n",
    "train_images = np.array(train_images[..., tf.newaxis], requires_grad=False)\n",
    "test_images = np.array(test_images[..., tf.newaxis], requires_grad=False)\n",
    "\n",
    "n_qubits = 4\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "# Random circuit parameters\n",
    "random_weights = np.random.uniform(high=2 * np.pi, size=(n_layers, n_qubits))\n",
    "\n",
    "@qml.qnode(dev, interface='tf')\n",
    "def qnode(inputs, weights):\n",
    "    inputs *= np.pi\n",
    "    # Encoding of 4 classical input values\n",
    "    qml.AngleEmbedding(inputs, wires=range(n_qubits), rotation='Y')\n",
    "\n",
    "    # Random quantum circuit\n",
    "    RandomLayers(weights, wires=list(range(n_qubits)))\n",
    "\n",
    "    # Measurement producing 4 classical output values\n",
    "    return [qml.expval(qml.PauliZ(j)) for j in range(n_qubits)]\n",
    "\n",
    "\n",
    "@qml.qnode(dev, interface='tf')\n",
    "def qnotnode(inputs):\n",
    "    inputs *= np.pi\n",
    "    # Encoding of 4 classical input values\n",
    "    qml.AngleEmbedding(inputs, wires=range(n_qubits), rotation='Y')\n",
    "\n",
    "    # Filter from arxiv.org/abs/2308.14930\n",
    "\n",
    "    #qml.CNOT(wires=[1, 2])\n",
    "    #qml.CNOT(wires=[0, 3])\n",
    "\n",
    "\n",
    "    # Measurement producing 4 classical output values\n",
    "    return [qml.expval(qml.PauliZ(j)) for j in range(n_qubits)]\n",
    "\n",
    "@qml.qnode(dev, interface='tf')\n",
    "def qhadrandnode2(inputs):\n",
    "    inputs *= np.pi\n",
    "    # Encoding of 4 classical input values\n",
    "    qml.AngleEmbedding(inputs, wires=range(n_qubits), rotation='Y')\n",
    "    \n",
    "    qml.Hadamard(wires=[1])\n",
    "    qml.Hadamard(wires=[0])\n",
    "    # Filter from arxiv.org/abs/2308.14930\n",
    "    qml.ctrl(qml.Hadamard, control=1)(wires=2)\n",
    "    qml.ctrl(qml.Hadamard, control=0)(wires=3)\n",
    "    qml.CNOT(wires=[3, 0])\n",
    "    qml.Hadamard(wires=[3])\n",
    "    qml.ctrl(qml.Hadamard, control=3)(wires=2)\n",
    "\n",
    "    # Measurement producing 4 classical output values\n",
    "    return [qml.expval(qml.PauliZ(j)) for j in range(n_qubits)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvQLayer(qml.qnn.KerasLayer):\n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        out = tf.Variable(tf.zeros((n_batches, 14, 14, n_qubits)))\n",
    "        for b in range(n_batches):\n",
    "            # Loop over the coordinates of the top-left pixel of 2X2 squares\n",
    "            for j in range(0, 28, 2):\n",
    "                for k in range(0, 28, 2):\n",
    "                    # Process a squared 2x2 region of the image with a quantum circuit\n",
    "                    q_results = tf.stack(\n",
    "                        [\n",
    "                            inputs[b,j, k, 0],\n",
    "                            inputs[b,j, k + 1, 0],\n",
    "                            inputs[b,j + 1, k, 0],\n",
    "                            inputs[b,j + 1, k + 1, 0]\n",
    "                        ],\n",
    "                        axis = 1\n",
    "                    )\n",
    "                    q_results = super().call(q_results)\n",
    "                    # Assign expectation values to different channels of the output pixel (j/2, k/2)\n",
    "                    for c in range(n_qubits):\n",
    "                        out[b,j // 2, k // 2, c] = q_results[c]\n",
    "        return out \n",
    "\n",
    "weight_shapes = {\"weights\": (n_layers, n_qubits)}\n",
    "\n",
    "qlayer = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=[28,28,n_qubits])\n",
    "\n",
    "qlayer.set_weights([random_weights]) \n",
    "\n",
    "qlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qnotlayer = qml.qnn.KerasLayer(qnotnode, {}, output_dim=[28,28,n_qubits])\n",
    "\n",
    "qnotlayer.trainable = False\n",
    "\n",
    "qhadrandlayer2 = qml.qnn.KerasLayer(qhadrandnode2, {}, output_dim=[28,28,n_qubits])\n",
    "\n",
    "qhadrandlayer2.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef Q_Model():\\n    \"\"\"Initializes and returns a custom Keras model\\n    which is ready to be trained.\"\"\"\\n    model = keras.models.Sequential([\\n        qlayer,\\n        keras.layers.Flatten(),\\n        keras.layers.Dense(10, activation=\"softmax\")\\n    ])\\n    model.compile(\\n        optimizer=\\'adam\\',\\n        loss=\"sparse_categorical_crossentropy\",\\n        metrics=[\"accuracy\"],\\n    )\\n    return model\\n\\nq_model = Q_Model()\\n\\nq_history = q_model.fit(\\n    train_images,\\n    train_labels,\\n    validation_data=(test_images, test_labels),\\n    batch_size = n_batches,\\n    epochs=n_epochs,\\n    verbose=2, callbacks=[tensorboard_callback, cp_callback],\\n    shuffle=True\\n)\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "def Q_Model():\n",
    "    \"\"\"Initializes and returns a custom Keras model\n",
    "    which is ready to be trained.\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        qlayer,\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "q_model = Q_Model()\n",
    "\n",
    "q_history = q_model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    batch_size = n_batches,\n",
    "    epochs=n_epochs,\n",
    "    verbose=2, callbacks=[tensorboard_callback, cp_callback],\n",
    "    shuffle=True\n",
    ")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef Qnot_Model():\\n    \"\"\"Initializes and returns a custom Keras model\\n    which is ready to be trained.\"\"\"\\n    model = keras.models.Sequential([\\n        qnotlayer,\\n        keras.layers.Flatten(),\\n        keras.layers.Dense(10, activation=\"softmax\")\\n    ])\\n    model.compile(\\n        optimizer=\\'adam\\',\\n        loss=\"sparse_categorical_crossentropy\",\\n        metrics=[\"accuracy\"]\\n    )\\n    return model\\n\\nqnot_model = Qnot_Model()\\n\\nqnot_history = qnot_model.fit(\\n    train_images,\\n    train_labels,\\n    validation_data=(test_images, test_labels),\\n    batch_size = n_batches,\\n    epochs=n_epochs,\\n    verbose=2, callbacks=[tensorboard_callback, cp_callback],\\n    shuffle=True\\n)\\n'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def Qnot_Model():\n",
    "    \"\"\"Initializes and returns a custom Keras model\n",
    "    which is ready to be trained.\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        qnotlayer,\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "qnot_model = Qnot_Model()\n",
    "\n",
    "qnot_history = qnot_model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    batch_size = n_batches,\n",
    "    epochs=n_epochs,\n",
    "    verbose=2, callbacks=[tensorboard_callback, cp_callback],\n",
    "    shuffle=True\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to ./training_2/cp-0001.ckpt\n",
      "50/50 - 3s - loss: 2.4586 - accuracy: 0.2100 - val_loss: 1.8760 - val_accuracy: 0.4333 - 3s/epoch - 67ms/step\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: saving model to ./training_2/cp-0002.ckpt\n",
      "50/50 - 3s - loss: 1.3141 - accuracy: 0.6300 - val_loss: 1.4977 - val_accuracy: 0.5417 - 3s/epoch - 64ms/step\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: saving model to ./training_2/cp-0003.ckpt\n",
      "50/50 - 3s - loss: 0.9352 - accuracy: 0.7250 - val_loss: 1.5451 - val_accuracy: 0.4000 - 3s/epoch - 61ms/step\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: saving model to ./training_2/cp-0004.ckpt\n",
      "50/50 - 3s - loss: 0.7296 - accuracy: 0.8050 - val_loss: 1.2442 - val_accuracy: 0.6333 - 3s/epoch - 66ms/step\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: saving model to ./training_2/cp-0005.ckpt\n",
      "50/50 - 3s - loss: 0.5356 - accuracy: 0.8700 - val_loss: 1.1325 - val_accuracy: 0.6000 - 3s/epoch - 63ms/step\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: saving model to ./training_2/cp-0006.ckpt\n",
      "50/50 - 3s - loss: 0.4542 - accuracy: 0.9100 - val_loss: 1.0248 - val_accuracy: 0.6750 - 3s/epoch - 62ms/step\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: saving model to ./training_2/cp-0007.ckpt\n",
      "50/50 - 3s - loss: 0.3964 - accuracy: 0.9150 - val_loss: 0.8712 - val_accuracy: 0.7000 - 3s/epoch - 63ms/step\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: saving model to ./training_2/cp-0008.ckpt\n",
      "50/50 - 3s - loss: 0.3672 - accuracy: 0.9200 - val_loss: 0.9498 - val_accuracy: 0.7333 - 3s/epoch - 61ms/step\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: saving model to ./training_2/cp-0009.ckpt\n",
      "50/50 - 3s - loss: 0.2927 - accuracy: 0.9500 - val_loss: 0.9170 - val_accuracy: 0.7167 - 3s/epoch - 58ms/step\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: saving model to ./training_2/cp-0010.ckpt\n",
      "50/50 - 3s - loss: 0.2670 - accuracy: 0.9550 - val_loss: 0.8492 - val_accuracy: 0.7000 - 3s/epoch - 66ms/step\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: saving model to ./training_2/cp-0011.ckpt\n",
      "50/50 - 3s - loss: 0.2160 - accuracy: 0.9550 - val_loss: 0.8558 - val_accuracy: 0.7417 - 3s/epoch - 60ms/step\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: saving model to ./training_2/cp-0012.ckpt\n",
      "50/50 - 3s - loss: 0.1918 - accuracy: 0.9700 - val_loss: 0.7694 - val_accuracy: 0.7417 - 3s/epoch - 62ms/step\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: saving model to ./training_2/cp-0013.ckpt\n",
      "50/50 - 3s - loss: 0.1981 - accuracy: 0.9800 - val_loss: 0.8226 - val_accuracy: 0.7000 - 3s/epoch - 61ms/step\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: saving model to ./training_2/cp-0014.ckpt\n",
      "50/50 - 3s - loss: 0.1267 - accuracy: 1.0000 - val_loss: 0.9022 - val_accuracy: 0.7000 - 3s/epoch - 62ms/step\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: saving model to ./training_2/cp-0015.ckpt\n",
      "50/50 - 3s - loss: 0.1532 - accuracy: 0.9850 - val_loss: 0.8230 - val_accuracy: 0.7333 - 3s/epoch - 63ms/step\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: saving model to ./training_2/cp-0016.ckpt\n",
      "50/50 - 3s - loss: 0.1147 - accuracy: 0.9950 - val_loss: 0.8020 - val_accuracy: 0.6917 - 3s/epoch - 61ms/step\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: saving model to ./training_2/cp-0017.ckpt\n",
      "50/50 - 3s - loss: 0.1057 - accuracy: 1.0000 - val_loss: 0.7857 - val_accuracy: 0.7583 - 3s/epoch - 62ms/step\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: saving model to ./training_2/cp-0018.ckpt\n",
      "50/50 - 3s - loss: 0.0970 - accuracy: 0.9950 - val_loss: 0.8833 - val_accuracy: 0.7167 - 3s/epoch - 64ms/step\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: saving model to ./training_2/cp-0019.ckpt\n",
      "50/50 - 3s - loss: 0.0850 - accuracy: 1.0000 - val_loss: 0.8880 - val_accuracy: 0.7250 - 3s/epoch - 62ms/step\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: saving model to ./training_2/cp-0020.ckpt\n",
      "50/50 - 3s - loss: 0.1029 - accuracy: 0.9950 - val_loss: 0.7912 - val_accuracy: 0.7250 - 3s/epoch - 64ms/step\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: saving model to ./training_2/cp-0021.ckpt\n",
      "50/50 - 3s - loss: 0.0857 - accuracy: 0.9950 - val_loss: 0.8021 - val_accuracy: 0.7333 - 3s/epoch - 62ms/step\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: saving model to ./training_2/cp-0022.ckpt\n",
      "50/50 - 3s - loss: 0.0644 - accuracy: 0.9950 - val_loss: 0.7838 - val_accuracy: 0.7583 - 3s/epoch - 62ms/step\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: saving model to ./training_2/cp-0023.ckpt\n",
      "50/50 - 3s - loss: 0.0633 - accuracy: 1.0000 - val_loss: 0.7475 - val_accuracy: 0.7333 - 3s/epoch - 62ms/step\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: saving model to ./training_2/cp-0024.ckpt\n",
      "50/50 - 3s - loss: 0.0580 - accuracy: 1.0000 - val_loss: 0.7761 - val_accuracy: 0.7750 - 3s/epoch - 67ms/step\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: saving model to ./training_2/cp-0025.ckpt\n",
      "50/50 - 3s - loss: 0.0493 - accuracy: 1.0000 - val_loss: 0.8256 - val_accuracy: 0.7333 - 3s/epoch - 62ms/step\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: saving model to ./training_2/cp-0026.ckpt\n",
      "50/50 - 3s - loss: 0.0577 - accuracy: 0.9950 - val_loss: 0.7619 - val_accuracy: 0.7500 - 3s/epoch - 64ms/step\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: saving model to ./training_2/cp-0027.ckpt\n",
      "50/50 - 3s - loss: 0.0457 - accuracy: 1.0000 - val_loss: 0.8053 - val_accuracy: 0.7500 - 3s/epoch - 66ms/step\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: saving model to ./training_2/cp-0028.ckpt\n",
      "50/50 - 3s - loss: 0.0493 - accuracy: 1.0000 - val_loss: 0.7976 - val_accuracy: 0.7583 - 3s/epoch - 63ms/step\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: saving model to ./training_2/cp-0029.ckpt\n",
      "50/50 - 3s - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.8013 - val_accuracy: 0.7333 - 3s/epoch - 66ms/step\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: saving model to ./training_2/cp-0030.ckpt\n",
      "50/50 - 3s - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.7599 - val_accuracy: 0.7500 - 3s/epoch - 62ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def Qhadrand2_Model():\n",
    "    \"\"\"Initializes and returns a custom Keras model\n",
    "    which is ready to be trained.\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        qhadrandlayer2,\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "qhadrand2_model = Qhadrand2_Model()\n",
    "\n",
    "qhadrand2_history = qhadrand2_model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    batch_size = n_batches,\n",
    "    epochs=n_epochs,\n",
    "    verbose=2, callbacks=[tensorboard_callback, cp_callback],\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML-QPF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
