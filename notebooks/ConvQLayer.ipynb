{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 01:46:44.601412: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-13 01:46:44.601459: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-13 01:46:44.602515: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-13 01:46:44.608535: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-13 01:46:45.324951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c6293b54f1aa306f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c6293b54f1aa306f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates import RandomLayers\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "%tensorboard --logdir logs/scalars/\n",
    "\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir= \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_grads=True\n",
    "    )\n",
    "\n",
    "checkpoint_path = \"./training_2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "   checkpoint_path, verbose=1, save_weights_only=True,\n",
    "   # Save weights, every epoch.\n",
    "   save_freq='epoch')\n",
    "\n",
    "\n",
    "n_epochs = 30   # Number of optimization epochs\n",
    "n_layers = 1    # Number of random layers\n",
    "n_train = 200    # Size of the train dataset\n",
    "n_test = 120     # Size of the test dataset\n",
    "n_batches = 4     # Size of the batches\n",
    "\n",
    "np.random.seed(0)           # Seed for NumPy random number generator\n",
    "tf.random.set_seed(0)       # Seed for TensorFlow random number generator\n",
    "\n",
    "\n",
    "mnist_dataset = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist_dataset.load_data()\n",
    "\n",
    "\n",
    "\n",
    "# Reduce dataset size\n",
    "train_images = train_images[:n_train]\n",
    "train_labels = train_labels[:n_train]\n",
    "test_images = test_images[:n_test]\n",
    "test_labels = test_labels[:n_test]\n",
    "\n",
    "tf.config.get_visible_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 01:46:49.315392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6795 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:b3:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Normalize pixel values within 0 and 1\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255\n",
    "\n",
    "# Add extra dimension for convolution channels\n",
    "train_images = np.array(train_images[..., tf.newaxis], requires_grad=False)\n",
    "test_images = np.array(test_images[..., tf.newaxis], requires_grad=False)\n",
    "\n",
    "n_qubits = 4\n",
    "\n",
    "dev = qml.device(\"default.qubit.tf\", wires=n_qubits)\n",
    "# Random circuit parameters\n",
    "random_weights = np.random.uniform(high=2 * np.pi, size=(n_layers, n_qubits))\n",
    "\n",
    "@qml.qnode(dev, interface='tf')\n",
    "def qnode(inputs):\n",
    "    inputs *= np.pi\n",
    "    # Encoding of 4 classical input values\n",
    "    qml.AngleEmbedding(inputs, wires=range(n_qubits), rotation='Y')\n",
    "\n",
    "    # Random quantum circuit\n",
    "    RandomLayers(random_weights, wires=list(range(n_qubits)))\n",
    "\n",
    "    # Measurement producing 4 classical output values\n",
    "    return [qml.expval(qml.PauliZ(j)) for j in range(n_qubits)]\n",
    "\n",
    "\n",
    "@qml.qnode(dev, interface='tf')\n",
    "def qnotnode(inputs):\n",
    "    inputs *= np.pi\n",
    "    # Encoding of 4 classical input values\n",
    "    qml.AngleEmbedding(inputs, wires=range(n_qubits), rotation='Y')\n",
    "\n",
    "    # Filter from arxiv.org/abs/2308.14930\n",
    "\n",
    "    qml.CNOT(wires=[1, 2])\n",
    "    qml.CNOT(wires=[0, 3])\n",
    "\n",
    "\n",
    "    # Measurement producing 4 classical output values\n",
    "    return [qml.expval(qml.PauliZ(j)) for j in range(n_qubits)]\n",
    "\n",
    "@qml.qnode(dev, interface='tf')\n",
    "def qhadrandnode(inputs):\n",
    "    inputs *= np.pi\n",
    "    # Encoding of 4 classical input values\n",
    "    qml.AngleEmbedding(inputs, wires=range(n_qubits), rotation='Y')\n",
    "\n",
    "\n",
    "    qml.CNOT(wires=[1, 2])\n",
    "    qml.CNOT(wires=[0, 3])\n",
    "    qml.Hadamard(wires=[1])\n",
    "    qml.Hadamard(wires=[2])\n",
    "    qml.ctrl(qml.Hadamard, control=1)(wires=2)\n",
    "    qml.ctrl(qml.Hadamard, control=2)(wires=0)\n",
    "    qml.ctrl(qml.Hadamard, control=1)(wires=2)\n",
    "    qml.ctrl(qml.Hadamard, control=0)(wires=3)\n",
    "    qml.Hadamard(wires=[1])\n",
    "    qml.Hadamard(wires=[0])\n",
    "    qml.Hadamard(wires=[3])\n",
    "\n",
    "    \n",
    "    # Measurement producing 4 classical output values\n",
    "    return [qml.expval(qml.PauliZ(j)) for j in range(n_qubits)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvQLayer(qml.qnn.KerasLayer):\n",
    "    \n",
    "    @tf.function(input_signature=(tf.TensorSpec(shape=[n_qubits, 3], dtype=tf.int64), tf.TensorSpec(shape=[28,28,1], dtype=tf.float32),))\n",
    "    def ops(self, op, subject):      \n",
    "      qnode_inputs = []\n",
    "      qnode_inputs = tf.gather_nd(subject,op)\n",
    "      return tf.dtypes.cast(super().call(qnode_inputs), tf.float32)\n",
    "\n",
    "\n",
    "    @tf.function(input_signature=(tf.TensorSpec(shape=[28, 28, 1], dtype=tf.float32),))\n",
    "    def squares(self, inputs):\n",
    "      return tf.reshape(\n",
    "        tf.map_fn(lambda x: self.ops(x, inputs), \n",
    "                        self.operations,\n",
    "                        parallel_iterations=196,\n",
    "                        fn_output_signature=tf.TensorSpec(shape=[n_qubits,], dtype=tf.float32)\n",
    "                        ),\n",
    "          [14,14,n_qubits])\n",
    "\n",
    "    @tf.function(input_signature=(tf.TensorSpec(shape=[n_batches,28,28,1], dtype=tf.float32),))\n",
    "    def batches(self, data):\n",
    "      return tf.map_fn(lambda x: self.squares(x), \n",
    "                      data,\n",
    "                        parallel_iterations=n_batches,\n",
    "                        fn_output_signature=tf.TensorSpec(shape=[14,14, n_qubits], dtype=tf.float32)\n",
    "                        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "      operation = []\n",
    "      for j in range(0, 28, 2):\n",
    "          for k in range(0, 28, 2):\n",
    "              operation.append(\n",
    "                  [\n",
    "                      [j, k, 0],\n",
    "                      [j, k + 1, 0],\n",
    "                      [j + 1, k, 0],\n",
    "                      [j + 1, k + 1, 0]\n",
    "                  ]\n",
    "              )\n",
    "      self.operations = tf.convert_to_tensor(np.asarray(operation))\n",
    "      return self.batches(inputs)\n",
    "\n",
    "qlayer = ConvQLayer(qhadrandnode, {}, output_dim=[14,14,n_qubits])\n",
    "\n",
    "qlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pre_Model():\n",
    "    \"\"\"Initializes and returns a custom Keras model\n",
    "    which is ready to be trained.\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        qlayer\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "pre_model = Pre_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _gcd_import at 0x7db9a170fd80> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function _gcd_import at 0x7db9a170fd80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _gcd_import at 0x7db9a170fd80> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function _gcd_import at 0x7db9a170fd80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "50/50 [==============================] - 132s 2s/step\n",
      "30/30 [==============================] - 74s 2s/step\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 01:50:15.844463: I external/local_xla/xla/service/service.cc:168] XLA service 0x7db7fbb87070 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-13 01:50:15.844509: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080, Compute Capability 7.5\n",
      "2024-05-13 01:50:15.849517: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-13 01:50:15.863526: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1715565015.981345    7256 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to ./training_2/cp-0001.ckpt\n",
      "50/50 - 1s - loss: 2.2888 - accuracy: 0.1750 - val_loss: 2.1185 - val_accuracy: 0.3167 - 954ms/epoch - 19ms/step\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: saving model to ./training_2/cp-0002.ckpt\n",
      "50/50 - 0s - loss: 1.7233 - accuracy: 0.5400 - val_loss: 1.7567 - val_accuracy: 0.5000 - 241ms/epoch - 5ms/step\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: saving model to ./training_2/cp-0003.ckpt\n",
      "50/50 - 0s - loss: 1.3157 - accuracy: 0.7750 - val_loss: 1.5780 - val_accuracy: 0.5167 - 283ms/epoch - 6ms/step\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: saving model to ./training_2/cp-0004.ckpt\n",
      "50/50 - 0s - loss: 1.0368 - accuracy: 0.8500 - val_loss: 1.4164 - val_accuracy: 0.6250 - 296ms/epoch - 6ms/step\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: saving model to ./training_2/cp-0005.ckpt\n",
      "50/50 - 0s - loss: 0.8671 - accuracy: 0.8400 - val_loss: 1.3305 - val_accuracy: 0.6250 - 265ms/epoch - 5ms/step\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: saving model to ./training_2/cp-0006.ckpt\n",
      "50/50 - 0s - loss: 0.7022 - accuracy: 0.9350 - val_loss: 1.2509 - val_accuracy: 0.6417 - 310ms/epoch - 6ms/step\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: saving model to ./training_2/cp-0007.ckpt\n",
      "50/50 - 0s - loss: 0.6029 - accuracy: 0.9350 - val_loss: 1.1867 - val_accuracy: 0.6167 - 313ms/epoch - 6ms/step\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: saving model to ./training_2/cp-0008.ckpt\n",
      "50/50 - 0s - loss: 0.5143 - accuracy: 0.9450 - val_loss: 1.1442 - val_accuracy: 0.6583 - 280ms/epoch - 6ms/step\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: saving model to ./training_2/cp-0009.ckpt\n",
      "50/50 - 0s - loss: 0.4498 - accuracy: 0.9500 - val_loss: 1.1531 - val_accuracy: 0.6250 - 310ms/epoch - 6ms/step\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: saving model to ./training_2/cp-0010.ckpt\n",
      "50/50 - 0s - loss: 0.4045 - accuracy: 0.9700 - val_loss: 1.0746 - val_accuracy: 0.7000 - 303ms/epoch - 6ms/step\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: saving model to ./training_2/cp-0011.ckpt\n",
      "50/50 - 0s - loss: 0.3534 - accuracy: 0.9750 - val_loss: 1.0662 - val_accuracy: 0.6583 - 255ms/epoch - 5ms/step\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: saving model to ./training_2/cp-0012.ckpt\n",
      "50/50 - 0s - loss: 0.3178 - accuracy: 0.9850 - val_loss: 1.0512 - val_accuracy: 0.6667 - 272ms/epoch - 5ms/step\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: saving model to ./training_2/cp-0013.ckpt\n",
      "50/50 - 0s - loss: 0.2800 - accuracy: 0.9850 - val_loss: 0.9981 - val_accuracy: 0.6833 - 254ms/epoch - 5ms/step\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: saving model to ./training_2/cp-0014.ckpt\n",
      "50/50 - 0s - loss: 0.2520 - accuracy: 0.9800 - val_loss: 1.0302 - val_accuracy: 0.6917 - 285ms/epoch - 6ms/step\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: saving model to ./training_2/cp-0015.ckpt\n",
      "50/50 - 0s - loss: 0.2389 - accuracy: 0.9850 - val_loss: 0.9851 - val_accuracy: 0.7000 - 290ms/epoch - 6ms/step\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: saving model to ./training_2/cp-0016.ckpt\n",
      "50/50 - 0s - loss: 0.2139 - accuracy: 0.9900 - val_loss: 0.9714 - val_accuracy: 0.7167 - 296ms/epoch - 6ms/step\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: saving model to ./training_2/cp-0017.ckpt\n",
      "50/50 - 0s - loss: 0.1915 - accuracy: 0.9950 - val_loss: 0.9709 - val_accuracy: 0.7000 - 277ms/epoch - 6ms/step\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: saving model to ./training_2/cp-0018.ckpt\n",
      "50/50 - 0s - loss: 0.1824 - accuracy: 0.9950 - val_loss: 0.9675 - val_accuracy: 0.7083 - 288ms/epoch - 6ms/step\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: saving model to ./training_2/cp-0019.ckpt\n",
      "50/50 - 0s - loss: 0.1639 - accuracy: 0.9950 - val_loss: 0.9907 - val_accuracy: 0.6833 - 267ms/epoch - 5ms/step\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: saving model to ./training_2/cp-0020.ckpt\n",
      "50/50 - 0s - loss: 0.1508 - accuracy: 1.0000 - val_loss: 0.9478 - val_accuracy: 0.7167 - 279ms/epoch - 6ms/step\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: saving model to ./training_2/cp-0021.ckpt\n",
      "50/50 - 0s - loss: 0.1405 - accuracy: 0.9950 - val_loss: 0.9528 - val_accuracy: 0.7250 - 295ms/epoch - 6ms/step\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: saving model to ./training_2/cp-0022.ckpt\n",
      "50/50 - 0s - loss: 0.1278 - accuracy: 0.9950 - val_loss: 0.9425 - val_accuracy: 0.7167 - 290ms/epoch - 6ms/step\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: saving model to ./training_2/cp-0023.ckpt\n",
      "50/50 - 0s - loss: 0.1207 - accuracy: 1.0000 - val_loss: 0.9391 - val_accuracy: 0.7167 - 276ms/epoch - 6ms/step\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: saving model to ./training_2/cp-0024.ckpt\n",
      "50/50 - 0s - loss: 0.1104 - accuracy: 1.0000 - val_loss: 0.9451 - val_accuracy: 0.7333 - 279ms/epoch - 6ms/step\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: saving model to ./training_2/cp-0025.ckpt\n",
      "50/50 - 0s - loss: 0.1038 - accuracy: 1.0000 - val_loss: 0.9372 - val_accuracy: 0.7250 - 245ms/epoch - 5ms/step\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: saving model to ./training_2/cp-0026.ckpt\n",
      "50/50 - 0s - loss: 0.0968 - accuracy: 1.0000 - val_loss: 0.9601 - val_accuracy: 0.7250 - 288ms/epoch - 6ms/step\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: saving model to ./training_2/cp-0027.ckpt\n",
      "50/50 - 0s - loss: 0.0919 - accuracy: 1.0000 - val_loss: 0.9481 - val_accuracy: 0.7417 - 235ms/epoch - 5ms/step\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: saving model to ./training_2/cp-0028.ckpt\n",
      "50/50 - 0s - loss: 0.0845 - accuracy: 1.0000 - val_loss: 0.9474 - val_accuracy: 0.7333 - 296ms/epoch - 6ms/step\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: saving model to ./training_2/cp-0029.ckpt\n",
      "50/50 - 0s - loss: 0.0800 - accuracy: 1.0000 - val_loss: 0.9461 - val_accuracy: 0.7333 - 254ms/epoch - 5ms/step\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: saving model to ./training_2/cp-0030.ckpt\n",
      "50/50 - 0s - loss: 0.0756 - accuracy: 1.0000 - val_loss: 0.9263 - val_accuracy: 0.7333 - 303ms/epoch - 6ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def Q_Model():\n",
    "    \"\"\"Initializes and returns a custom Keras model\n",
    "    which is ready to be trained.\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "train_images = pre_model.predict(train_images,batch_size=n_batches)\n",
    "test_images = pre_model.predict(test_images,batch_size=n_batches)\n",
    "q_model = Q_Model()\n",
    "\n",
    "q_history = q_model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    batch_size = n_batches,\n",
    "    epochs=n_epochs,\n",
    "    verbose=2, callbacks=[tensorboard_callback, cp_callback],\n",
    "    shuffle=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_titles = ['Five-Ch1', 'Five-Ch2', 'Five-Ch3','Five-Ch4']\n",
    "f, ax = plt.subplots(nrows=1, ncols=4, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(train_images[0][:,:,i])\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML-QPF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
