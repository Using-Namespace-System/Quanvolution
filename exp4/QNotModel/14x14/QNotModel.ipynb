{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 00:10:41.213802: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-14 00:10:41.213845: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-14 00:10:41.214825: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-14 00:10:41.220868: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-14 00:10:41.996607: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fd6169c574d4b411\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fd6169c574d4b411\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates import RandomLayers\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "#Hide GPU\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "%tensorboard --logdir logs/scalars/\n",
    "\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir= \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_grads=True\n",
    "    )\n",
    "\n",
    "checkpoint_path = \"./training_2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "   checkpoint_path, verbose=1, save_weights_only=True,\n",
    "   # Save weights, every epoch.\n",
    "   save_freq='epoch')\n",
    "\n",
    "\n",
    "n_epochs = 30   # Number of optimization epochs\n",
    "n_layers = 1    # Number of random layers\n",
    "n_train = 60000    # Size of the train dataset\n",
    "n_test = 10000    # Size of the test dataset\n",
    "n_batches = 4     # Size of the batches\n",
    "\n",
    "np.random.seed(0)           # Seed for NumPy random number generator\n",
    "tf.random.set_seed(0)       # Seed for TensorFlow random number generator\n",
    "\n",
    "\n",
    "mnist_dataset = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist_dataset.load_data()\n",
    "\n",
    "\n",
    "\n",
    "# Reduce dataset size\n",
    "train_images = train_images[:n_train]\n",
    "train_labels = train_labels[:n_train]\n",
    "test_images = test_images[:n_test]\n",
    "test_labels = test_labels[:n_test]\n",
    "\n",
    "\n",
    "\n",
    "tf.config.get_visible_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize pixel values within 0 and 1\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255\n",
    "\n",
    "# Add extra dimension for convolution channels\n",
    "train_images = np.array(train_images[..., tf.newaxis], requires_grad=False)\n",
    "test_images = np.array(test_images[..., tf.newaxis], requires_grad=False)\n",
    "\n",
    "n_qubits = 4\n",
    "\n",
    "dev = qml.device(\"default.qubit.tf\", wires=n_qubits)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.utils.register_keras_serializable()\n",
    "class ConvQLayer(qml.qnn.KerasLayer):\n",
    "    \n",
    "    @qml.qnode(dev, interface='tf')\n",
    "    def qnotnode(inputs):\n",
    "        inputs *= np.pi\n",
    "        # Encoding of 4 classical input values\n",
    "        qml.AngleEmbedding(inputs, wires=range(n_qubits), rotation='Y')\n",
    "\n",
    "        # Filter from arxiv.org/abs/2308.14930\n",
    "\n",
    "        qml.CNOT(wires=[1, 2])\n",
    "        qml.CNOT(wires=[0, 3])\n",
    "\n",
    "\n",
    "        # Measurement producing 4 classical output values\n",
    "        return [qml.expval(qml.PauliZ(j)) for j in range(n_qubits)]\n",
    "          \n",
    "\n",
    "\n",
    "    def __init__(self,qnode=None, weight_shapes={}, output_dim=[14,14,n_qubits],  *args, **kwargs):\n",
    "       super().__init__(qnode=self.qnotnode,weight_shapes=weight_shapes, output_dim=output_dim, *args, **kwargs)\n",
    "       \n",
    "       \n",
    "    \n",
    "    @tf.function(input_signature=(tf.TensorSpec(shape=[n_qubits, 3], dtype=tf.int64), tf.TensorSpec(shape=[28,28,1], dtype=tf.float32),))\n",
    "    def ops(self, op, subject):      \n",
    "      qnode_inputs = []\n",
    "      qnode_inputs = tf.gather_nd(subject,op)\n",
    "      return tf.dtypes.cast(super().call(qnode_inputs), tf.float32)\n",
    "\n",
    "\n",
    "    @tf.function(input_signature=(tf.TensorSpec(shape=[28, 28, 1], dtype=tf.float32),))\n",
    "    def squares(self, inputs):\n",
    "      return tf.reshape(\n",
    "        tf.map_fn(lambda x: self.ops(x, inputs), \n",
    "                        self.operations,\n",
    "                        parallel_iterations=196,\n",
    "                        fn_output_signature=tf.TensorSpec(shape=[n_qubits,], dtype=tf.float32)\n",
    "                        ),\n",
    "          [14,14,n_qubits])\n",
    "\n",
    "    @tf.function(input_signature=(tf.TensorSpec(shape=[n_batches,28,28,1], dtype=tf.float32),))\n",
    "    def batches(self, data):\n",
    "      return tf.map_fn(lambda x: self.squares(x), \n",
    "                      data,\n",
    "                        parallel_iterations=n_batches,\n",
    "                        fn_output_signature=tf.TensorSpec(shape=[14,14, n_qubits], dtype=tf.float32)\n",
    "                        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "      operation = []\n",
    "      for j in range(0, 28, 2):\n",
    "          for k in range(0, 28, 2):\n",
    "              operation.append(\n",
    "                  [\n",
    "                      [j, k, 0],\n",
    "                      [j, k + 1, 0],\n",
    "                      [j + 1, k, 0],\n",
    "                      [j + 1, k + 1, 0]\n",
    "                  ]\n",
    "              )\n",
    "      self.operations = tf.convert_to_tensor(np.asarray(operation))\n",
    "      return self.batches(inputs)\n",
    "\n",
    "qlayer = ConvQLayer()\n",
    "\n",
    "qlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pre_Model():\n",
    "    \"\"\"Initializes and returns a custom Keras model\n",
    "    which is ready to be trained.\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        qlayer\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "pre_model = Pre_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _gcd_import at 0x70f32a6efd80> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function _gcd_import at 0x70f32a6efd80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _gcd_import at 0x70f32a6efd80> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function _gcd_import at 0x70f32a6efd80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "15000/15000 [==============================] - 560s 37ms/step\n",
      "2500/2500 [==============================] - 91s 37ms/step\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 00:21:43.935981: I external/local_xla/xla/service/service.cc:168] XLA service 0x70f178068d90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-14 00:21:43.936014: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-05-14 00:21:43.942683: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1715646103.970424    6906 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-05-14 00:21:43.971828: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
      "2024-05-14 00:21:43.971949: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to ./training_2/cp-0001.ckpt\n",
      "15000/15000 - 28s - loss: 0.3485 - accuracy: 0.8949 - val_loss: 0.2355 - val_accuracy: 0.9309 - 28s/epoch - 2ms/step\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: saving model to ./training_2/cp-0002.ckpt\n",
      "15000/15000 - 27s - loss: 0.2494 - accuracy: 0.9276 - val_loss: 0.2233 - val_accuracy: 0.9366 - 27s/epoch - 2ms/step\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: saving model to ./training_2/cp-0003.ckpt\n",
      "15000/15000 - 26s - loss: 0.2331 - accuracy: 0.9324 - val_loss: 0.1997 - val_accuracy: 0.9456 - 26s/epoch - 2ms/step\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: saving model to ./training_2/cp-0004.ckpt\n",
      "15000/15000 - 27s - loss: 0.2244 - accuracy: 0.9358 - val_loss: 0.2185 - val_accuracy: 0.9372 - 27s/epoch - 2ms/step\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: saving model to ./training_2/cp-0005.ckpt\n",
      "15000/15000 - 28s - loss: 0.2179 - accuracy: 0.9376 - val_loss: 0.2279 - val_accuracy: 0.9342 - 28s/epoch - 2ms/step\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: saving model to ./training_2/cp-0006.ckpt\n",
      "15000/15000 - 26s - loss: 0.2164 - accuracy: 0.9395 - val_loss: 0.2216 - val_accuracy: 0.9392 - 26s/epoch - 2ms/step\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: saving model to ./training_2/cp-0007.ckpt\n",
      "15000/15000 - 28s - loss: 0.2103 - accuracy: 0.9414 - val_loss: 0.2738 - val_accuracy: 0.9287 - 28s/epoch - 2ms/step\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: saving model to ./training_2/cp-0008.ckpt\n",
      "15000/15000 - 28s - loss: 0.2094 - accuracy: 0.9417 - val_loss: 0.2329 - val_accuracy: 0.9385 - 28s/epoch - 2ms/step\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: saving model to ./training_2/cp-0009.ckpt\n",
      "15000/15000 - 26s - loss: 0.2090 - accuracy: 0.9427 - val_loss: 0.2454 - val_accuracy: 0.9375 - 26s/epoch - 2ms/step\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: saving model to ./training_2/cp-0010.ckpt\n",
      "15000/15000 - 28s - loss: 0.2083 - accuracy: 0.9426 - val_loss: 0.3027 - val_accuracy: 0.9200 - 28s/epoch - 2ms/step\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: saving model to ./training_2/cp-0011.ckpt\n",
      "15000/15000 - 29s - loss: 0.2058 - accuracy: 0.9434 - val_loss: 0.2361 - val_accuracy: 0.9397 - 29s/epoch - 2ms/step\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: saving model to ./training_2/cp-0012.ckpt\n",
      "15000/15000 - 26s - loss: 0.2067 - accuracy: 0.9442 - val_loss: 0.2535 - val_accuracy: 0.9341 - 26s/epoch - 2ms/step\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: saving model to ./training_2/cp-0013.ckpt\n",
      "15000/15000 - 27s - loss: 0.2041 - accuracy: 0.9442 - val_loss: 0.2734 - val_accuracy: 0.9311 - 27s/epoch - 2ms/step\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: saving model to ./training_2/cp-0014.ckpt\n",
      "15000/15000 - 28s - loss: 0.2025 - accuracy: 0.9457 - val_loss: 0.2423 - val_accuracy: 0.9389 - 28s/epoch - 2ms/step\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: saving model to ./training_2/cp-0015.ckpt\n",
      "15000/15000 - 26s - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.3346 - val_accuracy: 0.9226 - 26s/epoch - 2ms/step\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: saving model to ./training_2/cp-0016.ckpt\n",
      "15000/15000 - 27s - loss: 0.2044 - accuracy: 0.9446 - val_loss: 0.2499 - val_accuracy: 0.9371 - 27s/epoch - 2ms/step\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: saving model to ./training_2/cp-0017.ckpt\n",
      "15000/15000 - 28s - loss: 0.1996 - accuracy: 0.9453 - val_loss: 0.2577 - val_accuracy: 0.9384 - 28s/epoch - 2ms/step\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: saving model to ./training_2/cp-0018.ckpt\n",
      "15000/15000 - 27s - loss: 0.2008 - accuracy: 0.9467 - val_loss: 0.2423 - val_accuracy: 0.9440 - 27s/epoch - 2ms/step\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: saving model to ./training_2/cp-0019.ckpt\n",
      "15000/15000 - 26s - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.2490 - val_accuracy: 0.9435 - 26s/epoch - 2ms/step\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: saving model to ./training_2/cp-0020.ckpt\n",
      "15000/15000 - 28s - loss: 0.1993 - accuracy: 0.9467 - val_loss: 0.2567 - val_accuracy: 0.9380 - 28s/epoch - 2ms/step\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: saving model to ./training_2/cp-0021.ckpt\n",
      "15000/15000 - 28s - loss: 0.1999 - accuracy: 0.9470 - val_loss: 0.2898 - val_accuracy: 0.9329 - 28s/epoch - 2ms/step\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: saving model to ./training_2/cp-0022.ckpt\n",
      "15000/15000 - 26s - loss: 0.1977 - accuracy: 0.9477 - val_loss: 0.2227 - val_accuracy: 0.9495 - 26s/epoch - 2ms/step\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: saving model to ./training_2/cp-0023.ckpt\n",
      "15000/15000 - 29s - loss: 0.1985 - accuracy: 0.9470 - val_loss: 0.2373 - val_accuracy: 0.9459 - 29s/epoch - 2ms/step\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: saving model to ./training_2/cp-0024.ckpt\n",
      "15000/15000 - 28s - loss: 0.1990 - accuracy: 0.9475 - val_loss: 0.2447 - val_accuracy: 0.9427 - 28s/epoch - 2ms/step\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: saving model to ./training_2/cp-0025.ckpt\n",
      "15000/15000 - 26s - loss: 0.1980 - accuracy: 0.9470 - val_loss: 0.3323 - val_accuracy: 0.9273 - 26s/epoch - 2ms/step\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: saving model to ./training_2/cp-0026.ckpt\n",
      "15000/15000 - 27s - loss: 0.1968 - accuracy: 0.9483 - val_loss: 0.2726 - val_accuracy: 0.9385 - 27s/epoch - 2ms/step\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: saving model to ./training_2/cp-0027.ckpt\n",
      "15000/15000 - 26s - loss: 0.1962 - accuracy: 0.9478 - val_loss: 0.2613 - val_accuracy: 0.9404 - 26s/epoch - 2ms/step\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: saving model to ./training_2/cp-0028.ckpt\n",
      "15000/15000 - 27s - loss: 0.1988 - accuracy: 0.9479 - val_loss: 0.2919 - val_accuracy: 0.9343 - 27s/epoch - 2ms/step\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: saving model to ./training_2/cp-0029.ckpt\n",
      "15000/15000 - 28s - loss: 0.1974 - accuracy: 0.9486 - val_loss: 0.3258 - val_accuracy: 0.9308 - 28s/epoch - 2ms/step\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: saving model to ./training_2/cp-0030.ckpt\n",
      "15000/15000 - 27s - loss: 0.1963 - accuracy: 0.9492 - val_loss: 0.3156 - val_accuracy: 0.9316 - 27s/epoch - 2ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def Q_Model():\n",
    "    \"\"\"Initializes and returns a custom Keras model\n",
    "    which is ready to be trained.\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "q_model = Q_Model()\n",
    "\n",
    "pre_train_images = pre_model.predict(train_images,batch_size=n_batches)\n",
    "pre_test_images = pre_model.predict(test_images,batch_size=n_batches)\n",
    "\n",
    "q_history = q_model.fit(\n",
    "    pre_train_images,\n",
    "    train_labels,\n",
    "    validation_data=(pre_test_images, test_labels),\n",
    "    batch_size = n_batches,\n",
    "    epochs=n_epochs,\n",
    "    verbose=2, callbacks=[tensorboard_callback, cp_callback],\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "image_titles = ['Five-Ch1', 'Five-Ch2', 'Five-Ch3','Five-Ch4']\n",
    "f, ax = plt.subplots(nrows=1, ncols=4, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(pre_train_images[0][:,:,i])\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 71ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "full_model = keras.models.Sequential([pre_model,q_model])\n",
    "\n",
    "pred = full_model.predict(train_images[:n_batches], batch_size=n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:n_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.save('qnotmod.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model('qnotmod.keras')\n",
    "\n",
    "np.argmax(model.predict(train_images[:n_batches], batch_size=n_batches), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML-QPF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
